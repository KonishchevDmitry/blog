<!DOCTYPE html>
<html lang="ru" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Мониторим потребление памяти в Linux-системе | Dmitry Konishchev&#39;s small blog</title>
<meta name="keywords" content="">
<meta name="description" content="Сколько себя помню, меня всегда привлекали счётчики памяти в Linux: смотришь в условный htop – в плане потребления CPU вроде всё &#43;/- понятно, а вот память всегда считалась как-то не так, как ты это на первый взгляд ожидаешь, и долгое время у меня было довольно наивное и ошибочное представление о механизмах её работы.
Со временем некоторые вещи прояснялись, приходило понимание, как именно оно работает под капотом (до определённой степени). В какой-то момент возникла рабочая необходимость понять, куда уходит память на реальной системе – и этот случай в очередной раз показал, что местами оно устроено довольно неочевидно, и на этот вопрос не всегда просто дать ответ. Ну а помимо рабочей необходимости у меня дома давно стоит сервер, обвешанный метриками, и всегда хотелось высветить себе их в понятной форме, чтобы потом в реальном времени наблюдать, как ведёт себя система, когда в ней происходят те или иные процессы.">
<meta name="author" content="">
<link rel="canonical" href="https://konishchev.ru/posts/linux-memory-monitoring/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.27b0fc6a9e4df4110536f685cccf8ca9977c200af0d8eceaa8e77a5d07235262.css" integrity="sha256-J7D8ap5N9BEFNvaFzM&#43;MqZd8IArw2OzqqOd6XQcjUmI=" rel="preload stylesheet" as="style">

<!-- Console icon by Icons8 (https://icons8.com/icon/pSUzrDn0xTlh/console) -->

<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/32.png">
<link rel="icon" type="image/png" sizes="72x72" href="/images/favicon/72.png">
<link rel="icon" type="image/png" sizes="96x96" href="/images/favicon/96.png">

<link rel="apple-touch-icon" type="image/png" sizes="57x57" href="/images/favicon/57.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="/images/favicon/60.png">
<link rel="apple-touch-icon" type="image/png" sizes="72x72" href="/images/favicon/72.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="/images/favicon/76.png">


<link rel="alternate" hreflang="ru" href="https://konishchev.ru/posts/linux-memory-monitoring/">

<meta name="twitter:title" content="Мониторим потребление памяти в Linux-системе | Dmitry Konishchev&#39;s small blog" />
<meta name="twitter:description" content="Сколько себя помню, меня всегда привлекали счётчики памяти в Linux: смотришь в условный htop – в плане потребления CPU вроде всё &#43;/- понятно, а вот память всегда считалась как-то не так, как ты это на первый взгляд ожидаешь, и долгое время у меня было довольно наивное и ошибочное представление о механизмах её работы.
Со временем некоторые вещи прояснялись, приходило понимание, как именно оно работает под капотом (до определённой степени). В какой-то момент возникла рабочая необходимость понять, куда уходит память на реальной системе – и этот случай в очередной раз показал, что местами оно устроено довольно неочевидно, и на этот вопрос не всегда просто дать ответ. Ну а помимо рабочей необходимости у меня дома давно стоит сервер, обвешанный метриками, и всегда хотелось высветить себе их в понятной форме, чтобы потом в реальном времени наблюдать, как ведёт себя система, когда в ней происходят те или иные процессы." />
<meta property="og:title" content="Мониторим потребление памяти в Linux-системе | Dmitry Konishchev&#39;s small blog" />
<meta property="og:description" content="Сколько себя помню, меня всегда привлекали счётчики памяти в Linux: смотришь в условный htop – в плане потребления CPU вроде всё &#43;/- понятно, а вот память всегда считалась как-то не так, как ты это на первый взгляд ожидаешь, и долгое время у меня было довольно наивное и ошибочное представление о механизмах её работы.
Со временем некоторые вещи прояснялись, приходило понимание, как именно оно работает под капотом (до определённой степени). В какой-то момент возникла рабочая необходимость понять, куда уходит память на реальной системе – и этот случай в очередной раз показал, что местами оно устроено довольно неочевидно, и на этот вопрос не всегда просто дать ответ. Ну а помимо рабочей необходимости у меня дома давно стоит сервер, обвешанный метриками, и всегда хотелось высветить себе их в понятной форме, чтобы потом в реальном времени наблюдать, как ведёт себя система, когда в ней происходят те или иные процессы." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://konishchev.ru/posts/linux-memory-monitoring/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2025-04-02T19:22:25&#43;03:00" />
  <meta property="article:modified_time" content="2025-08-03T15:05:12&#43;03:00" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://konishchev.ru/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Мониторим потребление памяти в Linux-системе",
      "item": "https://konishchev.ru/posts/linux-memory-monitoring/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Мониторим потребление памяти в Linux-системе | Dmitry Konishchev's small blog",
  "name": "Мониторим потребление памяти в Linux-системе",
  "description": "Сколько себя помню, меня всегда привлекали счётчики памяти в Linux: смотришь в условный htop – в плане потребления CPU вроде всё +/- понятно, а вот память всегда считалась как-то не так, как ты это на первый взгляд ожидаешь, и долгое время у меня было довольно наивное и ошибочное представление о механизмах её работы.\nСо временем некоторые вещи прояснялись, приходило понимание, как именно оно работает под капотом (до определённой степени). В какой-то момент возникла рабочая необходимость понять, куда уходит память на реальной системе – и этот случай в очередной раз показал, что местами оно устроено довольно неочевидно, и на этот вопрос не всегда просто дать ответ. Ну а помимо рабочей необходимости у меня дома давно стоит сервер, обвешанный метриками, и всегда хотелось высветить себе их в понятной форме, чтобы потом в реальном времени наблюдать, как ведёт себя система, когда в ней происходят те или иные процессы.\n",
  "keywords": [
    
  ],
  "wordCount" : "3369",
  "inLanguage": "ru",
  "datePublished": "2025-04-02T19:22:25+03:00",
  "dateModified": "2025-08-03T15:05:12+03:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://konishchev.ru/posts/linux-memory-monitoring/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Dmitry Konishchev's small blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://konishchev.ru/favicon.ico"
    }
  }
}
</script>
<script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();
    for (var j = 0; j < document.scripts.length; j++) {if (document.scripts[j].src === r) { return; }}
    k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(96897335, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true
    });
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/96897335" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>

</head>

<body class=" type-posts kind-page layout-" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://konishchev.ru/" accesskey="h" title="Dmitry Konishchev&#39;s small blog (Alt + H)">Dmitry Konishchev&#39;s small blog</a>
            <span class="logo-switches">
            </span>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">Мониторим потребление памяти в Linux-системе</h1>
    <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>2 апреля 2025</span></span>
&nbsp;|&nbsp;<span class="meta-item">
    <span class="edit-post">
        <a href="https://github.com/KonishchevDmitry/blog/tree/master/content/posts/2025-04-02-linux-memory-monitoring/index.md" rel="noopener noreferrer" target="_blank">Исправить опечатку</a>
    </span>
</span>

      
    </div>
  </header> <div class="toc side right">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Оглавление</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%d0%bb%d0%b8%d0%ba%d0%b1%d0%b5%d0%b7-%d0%bf%d0%be-%d0%be%d1%80%d0%b3%d0%b0%d0%bd%d0%b8%d0%b7%d0%b0%d1%86%d0%b8%d0%b8-%d0%bf%d0%b0%d0%bc%d1%8f%d1%82%d0%b8-%d0%b2-linux" aria-label="Ликбез по организации памяти в Linux">Ликбез по организации памяти в Linux</a><ul>
                        
                <li>
                    <a href="#free-%d0%bf%d0%b0%d0%bc%d1%8f%d1%82%d1%8c" aria-label="Free-память">Free-память</a></li>
                <li>
                    <a href="#page-cache" aria-label="Page cache">Page cache</a></li>
                <li>
                    <a href="#buffers" aria-label="Buffers">Buffers</a></li>
                <li>
                    <a href="#%d0%b0%d0%bd%d0%be%d0%bd%d0%b8%d0%bc%d0%bd%d0%b0%d1%8f-%d0%bf%d0%b0%d0%bc%d1%8f%d1%82%d1%8c" aria-label="Анонимная память">Анонимная память</a></li>
                <li>
                    <a href="#shmem-shared-memory" aria-label="shmem (shared memory)">shmem (shared memory)</a></li>
                <li>
                    <a href="#swap" aria-label="swap">swap</a></li>
                <li>
                    <a href="#page-tables" aria-label="Page tables">Page tables</a></li>
                <li>
                    <a href="#activeinactiveunevictable" aria-label="Active/inactive/unevictable">Active/inactive/unevictable</a></li>
                <li>
                    <a href="#slab-kmalloc-vmalloc" aria-label="slab, kmalloc, vmalloc">slab, kmalloc, vmalloc</a></li>
                <li>
                    <a href="#reclaimable-%d0%bf%d0%b0%d0%bc%d1%8f%d1%82%d1%8c-%d0%b8-memory-pressure" aria-label="Reclaimable-память и memory pressure">Reclaimable-память и memory pressure</a></li>
                <li>
                    <a href="#kernel-stack" aria-label="Kernel stack">Kernel stack</a></li></ul>
                </li>
                <li>
                    <a href="#%d0%bc%d0%be%d0%bd%d0%b8%d1%82%d0%be%d1%80%d0%b8%d0%bc-procmeminfo" aria-label="Мониторим /proc/meminfo">Мониторим /proc/meminfo</a><ul>
                        
                <li>
                    <a href="#memory-usage" aria-label="Memory usage">Memory usage</a></li>
                <li>
                    <a href="#used-memory" aria-label="Used memory">Used memory</a></li>
                <li>
                    <a href="#caches" aria-label="Caches">Caches</a></li>
                <li>
                    <a href="#unknown" aria-label="Unknown">Unknown</a></li>
                <li>
                    <a href="#available-memory" aria-label="Available memory">Available memory</a></li>
                <li>
                    <a href="#memory-swappiness" aria-label="Memory swappiness">Memory swappiness</a></li>
                <li>
                    <a href="#page-cache-writeback" aria-label="Page cache writeback">Page cache writeback</a></li>
                <li>
                    <a href="#swap-usage" aria-label="Swap usage">Swap usage</a></li>
                <li>
                    <a href="#zswap" aria-label="zswap">zswap</a></li>
                <li>
                    <a href="#%d1%80%d0%b5%d0%b7%d1%83%d0%bb%d1%8c%d1%82%d0%b0%d1%82" aria-label="Результат">Результат</a></li></ul>
                </li>
                <li>
                    <a href="#%d1%81%d0%be%d0%b3%d0%bb%d0%b0%d1%81%d0%be%d0%b2%d0%b0%d0%bd%d0%bd%d0%be%d1%81%d1%82%d1%8c-%d0%b4%d0%b0%d0%bd%d0%bd%d1%8b%d1%85" aria-label="Согласованность данных">Согласованность данных</a></li>
                <li>
                    <a href="#%d0%b7%d0%b0%d0%ba%d0%bb%d1%8e%d1%87%d0%b5%d0%bd%d0%b8%d0%b5" aria-label="Заключение">Заключение</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Сколько себя помню, меня всегда привлекали счётчики памяти в Linux: смотришь в условный <code>htop</code> – в плане потребления CPU вроде всё +/- понятно, а вот память всегда считалась как-то не так, как ты это на первый взгляд ожидаешь, и долгое время у меня было довольно наивное и ошибочное представление о механизмах её работы.</p>
<p>Со временем некоторые вещи прояснялись, приходило понимание, как именно оно работает под капотом (до определённой степени). В какой-то момент возникла рабочая необходимость понять, куда уходит память на реальной системе – и этот случай в очередной раз показал, что местами оно устроено довольно неочевидно, и на этот вопрос не всегда просто дать ответ. Ну а помимо рабочей необходимости у меня дома давно стоит сервер, обвешанный метриками, и всегда хотелось высветить себе их в понятной форме, чтобы потом в реальном времени наблюдать, как ведёт себя система, когда в ней происходят те или иные процессы.</p>
<p>В этой статье я попробую разобрать, как сделать такой мониторинг и как интерпретировать его результаты. Сразу оговорюсь, что никогда не занимался разработкой ядра – вся информация ниже исключительно из личного опыта, поверхностного чтения исходников ядра и обильного гугления. Поэтому не исключено, что где-то могу быть неточным или вовсе неправым, но будем надеяться, что не сильно.</p>
<h2 id="ликбез-по-организации-памяти-в-linux">Ликбез по организации памяти в Linux<a hidden class="anchor" aria-hidden="true" href="#ликбез-по-организации-памяти-в-linux">¶</a></h2>
<p>Смотреть на счётчики ядра без понимания, что именно они измеряют, нет никакого смысла – поэтому начнём с описания базовых принципов того, как оно вообще работает под капотом (упрощая – иначе нужно писать не статью, а целую книгу).</p>
<h3 id="free-память">Free-память<a hidden class="anchor" aria-hidden="true" href="#free-память">¶</a></h3>
<p>Пожалуй, первое, о чём стоит упомянуть – так это о том, что если вы посмотрите на систему, которая проработала какое-то ненулевое количество времени не вхолостую, то у неё, как правило, будет очень маленькое количество free-памяти. И это полностью нормально, т. к. под free-памятью Linux понимает именно память, которая полностью свободна, и в ней ничего не хранится. Но это слишком ценный ресурс, чтобы просто так простаивать без дела – и ядро всегда пытается использовать всю свободную память с пользой и занять её какими-нибудь кэшами, которые будут ускорять работу системы, но при необходимости всегда могут быть быстро освобождены. Поэтому, как правило, на большинстве систем всю свободную память занимает page cache.</p>
<h3 id="page-cache">Page cache<a hidden class="anchor" aria-hidden="true" href="#page-cache">¶</a></h3>
<p>Если вы напишете программу, которая записывает в файл какие-то данные, а затем эта (или даже другая) программа будет его читать, то можно заметить интересную особенность: даже если файл очень большой (гигабайты), но меньше объёма свободной памяти, то и операции записи, и операции чтения из файла будут происходить очень быстро – гораздо быстрее, чем может работать диск под ними.</p>
<p>Всё дело в том, что когда программа пишет данные на диск (выполняет системный вызов <a href="https://man7.org/linux/man-pages/man2/write.2.html">write(2)</a>), работа этого системного вызова как правило заключается в том, что он просто записывает данные в память – и сразу же возвращает управление. И уже только потом (асинхронно) ядро записывает эти данные на диск. При этом после записи данные как правило продолжают оставаться в памяти, и последующий вызов <a href="https://man7.org/linux/man-pages/man2/read.2.html">read(2)</a> на том же файле сможет считать их оттуда моментально, совершенно не обращаясь к диску.</p>
<p>Данная подсистема ядра называется page cache, и смысл её работы (упрощённо) следующий: при работе с любым блочным устройством все (если специально не попросить обратного – см <code>O_DIRECT</code> в <a href="https://man7.org/linux/man-pages/man2/open.2.html">open(2)</a>)  операции чтения и записи происходят через page cache. Если ядру необходимо считать какую-то информацию с диска, то оно считывает её в page cache страницами по 4 KiB, и уже затем доступ к данным происходит через него. При записи также сначала информация попадает в page cache, и только потом асинхронно сбрасывается на диск (если не попросить это сделать досрочно через вызов <a href="https://man7.org/linux/man-pages/man2/fsync.2.html">fsync(2)</a>). Это приводит к интересной особенности – в общем случае, когда нет memory pressure (о нём поговорим ниже), запись всегда происходит мгновенно (т. к. мы по сути пишем в память, а не на диск), а вот чтение может быть долгим (если файл ещё не закэширован в page cache).</p>
<p>Вообще, page cache – это невероятно универсальный и крутой механизм, который используется практически повсюду. К примеру:</p>
<ol>
<li>С помощью него можно за-<a href="https://man7.org/linux/man-pages/man2/mmap.2.html">mmap(2)</a>&lsquo;ить файл в память и работать с ним так, как будто вы работаете с обычной памятью. При этом ОС автоматом будет подгружать данные, когда вы обращаетесь к конкретному участку памяти и даже больше – ещё до того, как попытаетесь обратиться – за счёт механизмов prefetching.</li>
<li>Знаете ли вы, как программы загружаются на исполнение? Наивный (и на первый взгляд логичный) ответ звучал бы так, что ядро выделяет блок памяти и считывает туда бинарь с диска, а затем передает управление на исполнение. Но на самом деле всё устроено гораздо интереснее: когда вы запускаете какое-либо приложение, Linux по сути просто <a href="https://man7.org/linux/man-pages/man2/mmap.2.html">mmap(2)</a>&lsquo;ит бинарник в память процесса – и передаёт управление на нужную инструкцию, а дальше уже по мере того, как процессор прыгает по этим инструкциям, подгружает данные в page cache и отдаёт оттуда. Поэтому даже очень толстые бинари потребляют небольшое количество памяти, если по факту исполняется только небольшая часть их кода. Правда, есть и другой side effect: в случае memory pressure (см. ниже) система может сбросить page cache загруженных бинарей, и даже если у вас в системе нет swap-файла, у вас будут точно такие же задержки, как и в случае, когда ОС выгрузила ваши данные на диск.</li>
</ol>
<p>Очень рекомендую почитать <a href="https://biriukov.dev/docs/page-cache/0-linux-page-cache-for-sre/">Linux Page Cache for SRE</a> для более глубокого погружения в тему.</p>
<h3 id="buffers">Buffers<a hidden class="anchor" aria-hidden="true" href="#buffers">¶</a></h3>
<p>Вот этот счётчик, на удивление, является самым запутанным по сравнению со всеми остальными. Вся информация, которая сходу гуглится по нему, довольно противоречивая, а в документации и вовсе написано &ldquo;Relatively temporary storage for raw disk blocks shouldn&rsquo;t get tremendously large (20MB or so)&rdquo; – при том, что я регулярно вижу как он показывает гигабайты.</p>
<p>Но на самом деле всё довольно просто: в ранних версиях Linux ту работу, которую сейчас выполняет подсистема Page Cache, выполняли две отдельные подсистемы. Теперь это уже не так, но в <code>/proc/meminfo</code> сохранено старое поведение, и счётчики <code>Cached</code> и <code>Buffers</code> отображают статистику разного типа кэшей:</p>
<ul>
<li><code>Cached</code> отвечает за кэширование содержимого файлов, когда обращение к ним производится через файловую систему.</li>
<li><code>Buffers</code> же отвечает за кэширование всего остального: блоков, которые содержат в себе метаданные файловой системы, разбивки диска, а также просто raw-блоки, когда вы читаете диск напрямую.</li>
</ul>
<p>Другими словами:</p>
<ul>
<li><code>Cached</code> увеличивается в результате следующих команд: <code>cat /dev/urandom &gt; out</code>, <code>cat big_file &gt; /dev/null</code>;</li>
<li><code>Buffers</code> увеличивается в случае <code>ls -laR / &gt; /dev/null</code> и <code>dd if=/dev/sda of=/dev/null bs=10M status=progress</code>.</li>
</ul>
<h3 id="анонимная-память">Анонимная память<a hidden class="anchor" aria-hidden="true" href="#анонимная-память">¶</a></h3>
<p>Технически, page cache – это страницы памяти, которые кэшируют содержимое блочного устройства и таким образом привязаны к нему. В противоположность page cache&rsquo;у существует анонимная (anonymous) память, у которой нет никакого backing-файла, и которая существует сама по себе.</p>
<p>Если не вдаваться в лишние в данном случае детали, то правильнее всего будет сказать, что анонимная память – это по сути вся userspace-память процессов: стек, глобальные переменные и куча.</p>
<h3 id="shmem-shared-memory">shmem (shared memory)<a hidden class="anchor" aria-hidden="true" href="#shmem-shared-memory">¶</a></h3>
<p>Под shared memory подразумеваются следующие вещи:</p>
<ol>
<li>Анонимные блоки памяти, которые создаются с помощью <a href="https://man7.org/linux/man-pages/man2/mmap.2.html">mmap(2)</a> + <code>MAP_ANONYMOUS | MAP_SHARED</code> для совместного использования несколькими процессами.</li>
<li><code>tmpfs</code> – виртуальная файловая система, которая полностью находится в памяти. К примеру, <code>/run</code> – это <code>tmpfs</code>. Некоторые дистрибутивы также монтируют <code>tmpfs</code> в <code>/tmp</code> – поэтому никогда не сохраняйте туда большие файлы! Для этого существует <code>/var/tmp</code>, который всегда лежит на диске.</li>
<li>POSIX IPC API  (<a href="https://man7.org/linux/man-pages/man7/shm_overview.7.html">shm_overview(7)</a>,  <a href="https://man7.org/linux/man-pages/man7/sem_overview.7.html">sem_overview(7)</a>), которое на самом деле в Linux реализовано поверх того же <code>tmpfs</code>, который монтируется в <code>/dev/shm</code>.</li>
</ol>
<p>Кстати, интересный и очень неочевидный факт: в Linux все файловые системы работают поверх page cache&rsquo;а – и по этой причине всё, что вы размещаете в <code>tmpfs</code>, засчитывается как page cache. Просто это такой несколько необычный page cache – под которым нет никакого файла, а есть только &ldquo;закэшированные&rdquo; страницы в памяти.</p>
<h3 id="swap">swap<a hidden class="anchor" aria-hidden="true" href="#swap">¶</a></h3>
<p>swap – это опциональный раздел или файл на диске, куда ядро может выгружать анонимную и tmpfs-память, когда она долгое время не используется, либо когда ядро ощущает нехватку свободной памяти.</p>
<p>При этом, когда процесс обращается к памяти, которая выгружена в swap, ядро не сразу перемещает эту память из swap&rsquo;а в оперативную. Вначале оно просто подгружает нужные страницы в память, не удаляя их из swap&rsquo;а (чтобы в случае чего можно было быстро опять от них избавиться, если их не успеют поменять) – такое состояние страниц называется swap cached.</p>
<p>Есть ещё различные дополнительные варианты, в число которых входит <a href="https://wiki.archlinux.org/title/Zswap">zswap</a> – когда перед реальным swap&rsquo;ом строится in-memory cache, в котором выгруженные страницы хранятся в сжатом виде. Очень интересная вещь – рекомендую.</p>
<p>Вообще, тема (необходимости) swap&rsquo;а – довольно холиварная и тянет на отдельную статью. Поэтому я мог бы тут порекомендовать почитать, к примеру, <a href="https://chrisdown.name/2018/01/02/in-defence-of-swap.html">In defence of swap: common misconceptions</a>.</p>
<h3 id="page-tables">Page tables<a hidden class="anchor" aria-hidden="true" href="#page-tables">¶</a></h3>
<p>Как вы вероятно уже знаете, каждый процесс работает в собственном виртуальном пространстве памяти. Виртуальная память – это абстракция, реализованная на уровне железа (<a href="https://en.wikipedia.org/wiki/Memory_management_unit">MMU</a>): для каждого процесса ядро составляет таблицы отображения виртуальных адресов в физические, и во время свой работы процессор использует их, виртуализируя память для текущего процесса.</p>
<p>Данные таблицы – многоуровневые, чтобы минимизировать их размер для типичного случая, когда процесс использует лишь малую часть своего виртуального пространства – и если не принимать во внимание специфические случаи вроде того, когда куча процессов шарят между собой одни и те же куски памяти, то можно воспринимать page tables как фиксированный налог на используемую физическую память, а следовательно их размер всегда будет предсказуемым и относительно небольшим.</p>
<p>Более подробно про page tables можно почитать в <a href="https://docs.kernel.org/mm/page_tables.html">документации к ядру</a>.</p>
<h3 id="activeinactiveunevictable">Active/inactive/unevictable<a hidden class="anchor" aria-hidden="true" href="#activeinactiveunevictable">¶</a></h3>
<p>Вся анонимная, page cache и swap cache-память классифицируется на:</p>
<ul>
<li>active – страницы, к которым недавно производился доступ;</li>
<li>inactive – страницы, к которым давно никто не обращался;</li>
<li>unevictable – страницы анонимной памяти, которые нельзя выгрузить в swap. Как правило, это страницы, которые явно были залочены в памяти через вызов <a href="https://man7.org/linux/man-pages/man2/mlock.2.html">mlock(2)</a>.</li>
</ul>
<p>В случае нехватки памяти ядро старается в первую очередь избавляться от inactive-страниц и только потом уже переходить к active-страницам.</p>
<p>Упрощённо, деление на active/inactive происходит следующим образом:</p>
<ol>
<li>У каждой страницы есть <code>accessed</code>-бит (на уровне page table).</li>
<li>Если ядро обрабатывает доступ к странице по какой-либо причине, то проставляет этот флаг.</li>
<li>Если процесс обращается к странице, минуя ядро, то тогда этот флаг проставляет <a href="https://en.wikipedia.org/wiki/Memory_management_unit">MMU</a>.</li>
<li>Ядро периодически сканирует страницы и если видит выставленный <code>accessed</code>-бит, то зануляет его + promote&rsquo;ит данную страницу из inactive в active.</li>
</ol>
<p>Более подробно про этот механизм можно почитать в <a href="https://www.kernel.org/doc/html/v4.18/admin-guide/mm/idle_page_tracking.html#implementation-details">документации к ядру</a>.</p>
<p>При этом важно отметить, что не стоит воспринимать inactive-страницы как &ldquo;страницы, к которым не было доступа в течение N секунд&rdquo; – это работает не так. Цель деления на active/inactive не в том, чтобы иметь хронометрическую статистику по активности страниц, а в том, чтобы приоритизировать все имеющиеся страницы для reclaim&rsquo;а. Поэтому данное деление на самом деле очень условное и больше отражает относительную востребованность страниц между собой, нежели фактическую. Общее представление о механизме балансирования страниц между этими двумя списками можно получить <a href="https://biriukov.dev/docs/page-cache/4-page-cache-eviction-and-page-reclaim/#theory">тут</a>.</p>
<h3 id="slab-kmalloc-vmalloc">slab, kmalloc, vmalloc<a hidden class="anchor" aria-hidden="true" href="#slab-kmalloc-vmalloc">¶</a></h3>
<p>Чтобы выполнять свою работу, ядру необходимо где-то аллоцировать память под различные структуры, в которых оно хранит текущее состояние системы. Для этих целей в ядре есть <a href="https://hammertux.github.io/slab-allocator">slab-аллокатор</a>, суть которого заключается в следующем: под наиболее часто используемые структуры (пример – структура, описывающая процесс, <code>task_struct</code>) заводится свой пул, к которому можно обратиться и попросить аллоцировать память под новый объект. При этом когда мы возвращаем объект в пул, память не освобождается сразу (из расчёта, что через мгновение может прийти запрос на новую аллокацию), и таким образом в slab-аллокаторе, как правило, находится некоторое количество памяти, которую можно в любой момент быстро освободить, если она понадобится в другом месте.</p>
<p>Помимо этого, сам тип объекта в slab&rsquo;е может быть помечен как reclaimable – т. е. что он по сути является кэшем, и такие объекты можно при необходимости удалить. Пожалуй самый типичный пример таких объектов – это dentry/inode-кэши, которые, в отличие от page cache&rsquo;а, кэшируют уже не данные, а метаданные файлов и содержимое директорий. Благодаря им, когда вы делаете что-то вроде <code>open(&quot;/a/b/c/d/e&quot;, ...)</code>, системе не нужно каждый раз проходиться по всем директориям в поисках конечного файла (при этом кэшируются также и негативные результаты поиска).</p>
<p>Как правило, slab-аллокатор используется для структур, которые создаются часто и в большом количестве, но если нужно просто аллоцировать что-то по месту, то используется <code>kmalloc()</code> (по смыслу напоминающий <a href="https://man7.org/linux/man-pages/man3/malloc.3.html">malloc(3)</a>), который на самом деле является надстройкой над slab-аллокатором. А для выделения больших блоков памяти (но без гарантий физической последовательности страниц) используется <code>vmalloc()</code>, который уже никак не связан со slab&rsquo;ом и аллоцирует память напрямую из <a href="https://grimoire.carcano.ch/blog/memory-management-the-buddy-allocator/">buddy-аллокатора</a>.</p>
<p>Более подробно про типы аллокаторов можно почитать в <a href="https://www.kernel.org/doc/html/v5.0/core-api/memory-allocation.html">документации к ядру</a>. Посмотреть разбивку по текущему использованию slab&rsquo;а можно командой <code>slabtop -sc</code>.</p>
<h3 id="reclaimable-память-и-memory-pressure">Reclaimable-память и memory pressure<a hidden class="anchor" aria-hidden="true" href="#reclaimable-память-и-memory-pressure">¶</a></h3>
<p>В процессе своей работы ядро мониторит текущее состояние памяти и:</p>
<ol>
<li>Вытесняет неиспользуемую память в swap, чтобы её можно было использовать с большей пользой (к примеру, под page cache);</li>
<li>Проактивно освобождает память, если есть признаки того, что скоро она может закончиться;</li>
<li>Агрессивно освобождает память, если её осталось совсем мало.</li>
</ol>
<p>С этой целью в ядре:</p>
<ol>
<li>Есть конфигурируемые пороги на количество свободной памяти, которые триггерят проактивное и агрессивное освобождение (reclaim) памяти. Проактивное – когда память заранее reclaim&rsquo;ит выделенный background-тред; агрессивное – когда уже сами процессы вынуждены тратить своё процессорное время на reclaim, чтобы найти свободную страничку памяти.</li>
<li>Есть представление о том, что в случае нехватки памяти можно сбросить все кэши, о которых шла речь выше (правда, если это грязный page cache, то его сначала придётся записать на диск).</li>
<li>Есть деление на active и inactive-память, и ядро старается в первую очередь вытеснять inactive-страницы.</li>
</ol>
<p>Общий принцип работы memory pressure очень хорошо описан в <a href="https://www.yugabyte.com/blog/linux-performance-tuning-memory-disk-io/#linux-free-memory">Linux Performance Tuning: Dealing with Memory and Disk IO</a> + <a href="https://habr.com/ru/companies/otus/articles/765824/">перевод</a> – рекомендую почитать.</p>
<h3 id="kernel-stack">Kernel stack<a hidden class="anchor" aria-hidden="true" href="#kernel-stack">¶</a></h3>
<p>Как вы, вероятно, знаете, у каждого треда в рамках процесса есть свой стек. Но, что интересно, на самом деле у него их два – user space и kernel space. Помню, когда-то давно мне казалось, что системный вызов – это как вызов API некоторого сервиса: мы послали запрос, его положили в очередь на обработку, а нас поставили на паузу, пока на той стороне не найдут время его обработать. Но ядро работает не так. :) Когда вы делаете системный вызов, то происходит <a href="https://en.wikipedia.org/wiki/Context_switch">переключение контекста</a> – и по сути продолжается работа вашего треда – только уже выполняется код ядра, который обрабатывает системный вызов, и этому коду нужен свой стек для работы.</p>
<p>Кстати, потребление CPU как раз по этой причине делится на <code>user</code> и <code>system</code>: т. е. это и правда время работы конкретного процесса/треда (task в терминах ядра), но в разных контекстах.</p>
<h2 id="мониторим-procmeminfo">Мониторим /proc/meminfo<a hidden class="anchor" aria-hidden="true" href="#мониторим-procmeminfo">¶</a></h2>
<p>Под мониторингом в данном случае я подразумеваю просвечивание метрик в <a href="https://prometheus.io/">Prometheus</a> / <a href="https://victoriametrics.com/">VictoriaMetrics</a>, чтобы впоследствии можно было видеть, что происходило с системой в конкретный момент времени.</p>
<p>Сразу скажу, что не буду в рамках этой статьи пытаться ставить какие-то особо амбициозные цели замониторить всё, что только можно с максимальной детализацией – и ограничусь той информацией, которую даёт нам <code>/proc/meminfo</code>. <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a>, <a href="https://utcc.utoronto.ca/~cks/space/blog/linux/KernelMemoryZones">zones</a>, <a href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/performance_tuning_guide/s-memory-transhuge#s-memory-configure_hugepages">huge pages</a>, фрагментацию и пр. тоже учитывать не будем – это переусложнит задачу, а реальная необходимость в этом есть только в довольно специфичных случаях.</p>
<p>К счастью, в плане сбора метрик всё уже сделано за нас – <a href="https://github.com/prometheus/node_exporter">Prometheus Node Exporter</a> высвечивает все счётчики из <code>/proc/meminfo</code> в виде метрик <code>node_memory_*</code>. А вот с интерпретацией получившихся значений придётся поработать самим: во-первых, часть из них пересекается по учитываемым ими страницам памяти – и не всегда очевидно, что именно нужно вычитать/суммировать, чтобы получить разбивку, а во-вторых, документация к ним местами крайне расплывчатая, и сходу не всегда понятно, что именно они считают.</p>
<p>Поэтому вооружаемся <a href="https://github.com/torvalds/linux/blob/91fe0e4d044044592e667fd4784edc39fe53bbd8/Documentation/filesystems/proc.rst#meminfo">документацией</a>, <a href="https://github.com/torvalds/linux/blob/6e1fa555ec772046ec3b903f507ff7fed5323796/fs/proc/meminfo.c#L34">исходниками</a> и информацией, представленной выше – и идём смотреть, что же мы можем отсюда почерпнуть.</p>
<p>Ниже каждый заголовок представляет собой отдельный график, из которых мы будем формировать наш dashboard.</p>
<h3 id="memory-usage">Memory usage<a hidden class="anchor" aria-hidden="true" href="#memory-usage">¶</a></h3>
<p>Первым делом попробуем сделать график максимально общей картины с высоты птичьего полёта, на котором отделим &ldquo;реально используемую&rdquo; память от всякого рода кэшей:</p>
<ul>
<li><code>Free = MemFree</code></li>
<li><code>Caches = Cached - Shmem + Buffers + KReclaimable</code></li>
<li><code>Used = MemTotal - Free - Caches</code></li>
</ul>
<h3 id="used-memory">Used memory<a hidden class="anchor" aria-hidden="true" href="#used-memory">¶</a></h3>
<p>Тут детализируем всю память, которая не является кэшами:</p>
<ul>
<li><code>Anonymous = AnonPages</code></li>
<li><code>Slab = SUnreclaim</code></li>
<li><code>Swap cached = SwapCached</code></li>
<li><code>zswap = Zswap</code> – память, которую потребляет подсистема zswap</li>
<li><code>Page tables = PageTables + SecPageTables</code></li>
<li><code>Kernel stacks = KernelStack</code></li>
<li><code>vmalloc = VmallocUsed - KernelStack</code> (в современных ядрах kernel stack <a href="https://docs.kernel.org/mm/vmalloced-kernel-stacks.html">выделяется через vmalloc</a>)</li>
<li><code>percpu = Percpu</code></li>
<li><code>shmem = Shmem</code></li>
</ul>
<h3 id="caches">Caches<a hidden class="anchor" aria-hidden="true" href="#caches">¶</a></h3>
<p>Здесь более детально отобразим, какие именно кэши сколько у нас занимают:</p>
<ul>
<li><code>Page cache = Cached - Shmem</code></li>
<li><code>Buffers = Buffers</code></li>
<li><code>Slab = SReclaimable</code></li>
<li><code>Misc = KReclaimable - SReclaimable</code></li>
</ul>
<p>Запускаем <code>sync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches</code> (см. <a href="https://github.com/torvalds/linux/blob/609706855d90bcab6080ba2cd030b9af322a1f0c/Documentation/admin-guide/sysctl/vm.rst#drop_caches">документацию</a>) – и наблюдаем, как много из этих кэшей система может освободить на самом деле (спойлер: далеко не всё).</p>
<p>При этом стоит отметить, что может быть и обратная ситуация – когда reclaimable памяти на самом деле больше, чем видно на первый взгляд. Очень показательным примером такого случая являются <a href="https://blogs.oracle.com/linux/post/zombie-memcg-issues">zombie memory cgroups</a>.</p>
<h3 id="unknown">Unknown<a hidden class="anchor" aria-hidden="true" href="#unknown">¶</a></h3>
<p>Как бы нам ни хотелось детализировать абсолютно всю память, на 100% это сделать невозможно – просто потому, что некоторые аллокации делаются напрямую из buddy-аллокатора и не крутят никакие счётчики. Поэтому какая-то часть выделенной памяти (как правило, небольшая) будет вне наших подсчётов, и важно тут не забыть про неё:</p>
<p><code>Unknown = MemTotal - MemFree - (AnonPages + SwapCached + Zswap + SUnreclaim + VmallocUsed + PageTables + SecPageTables + Percpu) - (Cached + Buffers + KReclaimable)</code></p>
<p>Тут, кстати, будет не лишним уточнить, что же такое на самом деле <code>MemTotal</code>. При запуске ядро обнаруживает всю память, доступную в системе, и под каждую физическую страницу создаёт структуру <a href="https://github.com/torvalds/linux/blob/d7b8f8e20813f0179d8ef519541a3527e7661d3a/include/linux/mm_types.h#L73">struct page</a>, которыми затем оперирует buddy-аллокатор. <code>MemTotal</code> при этом отражает общий размер памяти, доступный для аллокации, т. е., грубо говоря, количество <code>struct page</code>, умноженное на размер страницы (и не включает в себя память, занятую самим buddy-аллокатором, а также зарезервированную под аппаратное обеспечение и память, в которой находится код ядра). Именно поэтому он всегда меньше размера физической памяти.</p>
<h3 id="available-memory">Available memory<a hidden class="anchor" aria-hidden="true" href="#available-memory">¶</a></h3>
<p>Здесь у нас будет всего одна метрика – <code>MemAvailable</code>. Она представляет из себя оценочное количество памяти, которое доступно приложениям для аллокации без ухода в swap.</p>
<p>Вычисляется она <a href="https://github.com/torvalds/linux/blob/e8c1a296b8066734ef20797ab77e03a90b0c9be8/mm/show_mem.c#L32">следующим образом</a>: берутся все свободные страницы, к ним добавляется весь page cache и reclaimable-память ядра + делаются поправки на watermark&rsquo;и memory pressure и тот факт, что абсолютно всю память по-reclaim&rsquo;ить всё равно не получится, т. к. по факту какая-то её часть всё равно нужна для нормальной работы системы.</p>
<h3 id="memory-swappiness">Memory swappiness<a hidden class="anchor" aria-hidden="true" href="#memory-swappiness">¶</a></h3>
<p>Тут попробуем отобразить, как ядро видит пользовательскую память с точки зрения возможности reclaim&rsquo;а:</p>
<ul>
<li><code>Active anonymous = Active(anon)</code></li>
<li><code>Inactive anonymous = Inactive(anon)</code></li>
<li><code>Active page cache = Active(file)</code></li>
<li><code>Inactive page cache = Inactive(file)</code></li>
<li><code>Unevictable = Unevictable</code></li>
</ul>
<h3 id="page-cache-writeback">Page cache writeback<a hidden class="anchor" aria-hidden="true" href="#page-cache-writeback">¶</a></h3>
<p>На этом графике будем наблюдать за объёмом грязного page cache&rsquo;а и тем, как система справляется с синхронизацией его страниц на диск.</p>
<ul>
<li><code>Writeback = Writeback</code> – объём грязного page cache&rsquo;а, который в данный момент пишется на диск</li>
<li><code>Dirty = Dirty</code> – грязные страницы, которые пока только ждут своей очереди</li>
</ul>
<h3 id="swap-usage">Swap usage<a hidden class="anchor" aria-hidden="true" href="#swap-usage">¶</a></h3>
<ul>
<li><code>Cashed = SwapCached</code> – закэшированные в памяти страницы swap&rsquo;а</li>
<li><code>zswapped = Zswapped</code> – объём памяти, выгруженный в zswap</li>
<li><code>Swapped out = SwapTotal - SwapFree - SwapCached - Zswapped</code> – выгруженная (и не закэшированная) на диск память</li>
</ul>
<h3 id="zswap">zswap<a hidden class="anchor" aria-hidden="true" href="#zswap">¶</a></h3>
<p>Тут у меня есть свой <a href="https://github.com/KonishchevDmitry/server-metrics/blob/347236b21cd7c3a923e3c932a28dae52de01e52e/internal/zswap/metrics.go">zswap exporter</a>, который позволяет дополнительно следить за:</p>
<ol>
<li>Степенью сжатия, которую мы на самом деле получаем;</li>
<li>Заполнением zswap-пула;</li>
<li>Причинами, по которым страницы пролетели мимо zswap.</li>
</ol>
<h3 id="результат">Результат<a hidden class="anchor" aria-hidden="true" href="#результат">¶</a></h3>
<p>В итоге получаем вот такую красоту:
<a href="dashboard.png"><img loading="lazy" src="/posts/linux-memory-monitoring/dashboard.png" type="" alt="Dashboard"  /></a></p>
<h2 id="согласованность-данных">Согласованность данных<a hidden class="anchor" aria-hidden="true" href="#согласованность-данных">¶</a></h2>
<p>Это может быть довольно неочевидным, но счётчики, которые выдаёт <code>/proc/meminfo</code> (наравне со многими другими файлами в proc/sysfs), могут быть несогласованными между собой. А именно: при сборе статистики ядро атомарно читает/считает отдельные значения, но не берёт какую-либо блокировку, которая гарантировала бы согласованность полученных значений. Это осознанный компромисс разработчиков ядра, чтобы сбор данных для мониторинга (для которого такая несогласованность обычно не критична) не замедлял работу системы. Поэтому стоит учитывать данный факт, если вы используете их для каких-то более ответственных целей, нежели рисования графиков (к примеру, при вычитании одних чисел из других периодически могут получаться отрицательные значения).</p>
<p>Наглядный пример: изменение <code>nr_huge_pages</code> + <code>surplus_huge_pages</code> + <code>free_huge_pages</code> <a href="https://github.com/torvalds/linux/blob/2942242dde896ea8544f321617c86f941899c544/mm/hugetlb.c#L3809">всегда производится под <code>hugetlb_lock</code></a>, но вот при сборе статистики ядро <a href="https://github.com/torvalds/linux/blob/2942242dde896ea8544f321617c86f941899c544/mm/hugetlb.c#L5231">полагается исключительно на атомарное чтение отдельных <code>long</code>-переменных</a>.</p>
<h2 id="заключение">Заключение<a hidden class="anchor" aria-hidden="true" href="#заключение">¶</a></h2>
<p>Выше мы с вами мониторили всю систему целиком. Это, безусловно, очень интересно, но сразу же возникает желание пойти дальше – и видеть, какой именно сервис триггерит то или иное движение на графиках. И это можно сделать благодаря systemd + cgroups! systemd нарезает всю систему на понятные группы процессов (у которых, в отличие от pid&rsquo;ов, есть человеческое имя и, что самое важное – их конечное количество) – и если вы готовы к кратному увеличению количества метрик, то можно мониторить каждый сервис в отдельности, что выводит observability вашей системы на качественно новый уровень.</p>
<p>В рамках данной статьи я не буду пытаться покрыть всё, до чего могу только дотянуться – поэтому лишь намекну на такую возможность – возможно когда-нибудь это станет темой очередной статьи. А пока лишь могу поделиться своим exporter&rsquo;ом, которым сам пользуюсь для этой цели – <a href="https://github.com/KonishchevDmitry/server-metrics">server-metrics</a>. На данный момент он писался без мысли о том, что им может пользоваться кто-то кроме меня, но возможно для кого-то он сможет послужить источником вдохновения или рабочим примером того, как можно организовать такой мониторинг.</p>


  </div>

  <footer class="post-footer">
  </footer>
    <div class="comments-separator"></div><script src="https://giscus.app/client.js"
        data-repo="KonishchevDmitry/blog"
        data-repo-id="R_kgDOLqoJ-g"
        data-category="Announcements"
        data-category-id="DIC_kwDOLqoJ-s4Cefph"
        data-mapping="pathname"
        data-strict="1"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer"><div class="social-icons">
    <a href="mailto:konishchev@gmail.com" target="_blank" title="Email" style="box-shadow: none">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path>
    <polyline points="22,6 12,13 2,6"></polyline>
</svg>

    </a>
    <a href="https://github.com/KonishchevDmitry" target="_blank" title="GitHub" style="box-shadow: none">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>

    </a>
    <a href="https://www.linkedin.com/in/konishchev/" target="_blank" title="LinkedIn" style="box-shadow: none">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>

    </a>
    <a href="/rss.xml" target="_blank" title="RSS" style="box-shadow: none">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M4 11a9 9 0 0 1 9 9" />
    <path d="M4 4a16 16 0 0 1 16 16" />
    <circle cx="5" cy="19" r="1" />
</svg>

    </a>
</div>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
    <path d="M12 6H0l6-6z" />
  </svg>
</a>



<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
    if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
      mybutton.style.visibility = "visible";
      mybutton.style.opacity = "1";
    } else {
      mybutton.style.visibility = "hidden";
      mybutton.style.opacity = "0";
    }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>
<script>
  document.querySelectorAll('pre > code').forEach((codeblock) => {
    const container = codeblock.parentNode.parentNode;

    const copybutton = document.createElement('button');
    copybutton.classList.add('copy-code');
    copybutton.innerText = 'копировать';

    function copyingDone() {
      copybutton.innerText = 'скопировано!';
      setTimeout(() => {
        copybutton.innerText = 'копировать';
      }, 2000);
    }

    copybutton.addEventListener('click', (cb) => {
      if ('clipboard' in navigator) {
        navigator.clipboard.writeText(codeblock.textContent);
        copyingDone();
        return;
      }

      const range = document.createRange();
      range.selectNodeContents(codeblock);
      const selection = window.getSelection();
      selection.removeAllRanges();
      selection.addRange(range);
      try {
        document.execCommand('copy');
        copyingDone();
      } catch (e) { };
      selection.removeRange(range);
    });

    if (container.classList.contains("highlight")) {
      container.appendChild(copybutton);
    } else if (container.parentNode.firstChild == container) {
      
    } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
      
      codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
    } else {
      
      codeblock.parentNode.appendChild(copybutton);
    }
  });
</script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>

</body>

</html>
